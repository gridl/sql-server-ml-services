{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- Data import\n",
    "- Styling\n",
    "- Metadata and data types\n",
    "- Duplicate detection\n",
    "- Dataframe manipulation\n",
    "- Outlier detection\n",
    "- Variable generation and manipulation\n",
    "- Preparation of data for modeling\n",
    "\n",
    "#### Facts\n",
    "- Pre-modeling vs modeling 80% vs 20% of work\n",
    "\n",
    "#### Libraries:\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Sci-kit learn\n",
    "- Matplotlib\n",
    "\n",
    "[Data Set - Adult](https://archive.ics.uci.edu/ml/datasets/Adult)\n",
    "\n",
    "[Machine Learning Databases (a lot of them!!!)](http://archive.ics.uci.edu/ml/machine-learning-databases/)\n",
    "\n",
    "[Main UCI page](http://archive.ics.uci.edu/ml)\n",
    "\n",
    "Boston Housing dataset - we will use the copy that comes with sklearn\n",
    "\n",
    "[Boston Housing at UCI page](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration -  Exploratory Data Anaysis (EDA)\n",
    "\n",
    "The best way is to look into data (visually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF = pd.DataFrame(df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.columns = df.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['Price'] = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a subset of the dataframe columns based on the column data types\n",
    "dataF.select_dtypes(include='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.select_dtypes(include='bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unique values\n",
    "\n",
    "list(set(dataF['ZN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and that way?\n",
    "dataF['ZN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and that way - v3?\n",
    "dataF['ZN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more visual approach\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataF, height=1.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code   | Description   |\n",
    "|:---|:---|\n",
    "|**ZN**  | proportion of residential land zoned for lots over 25,000 sq.ft. | \n",
    "|**INDUS**  | proportion of non-retail business acres per town | \n",
    "|**NOX**  | nitric oxides concentration (parts per 10 million) | \n",
    "|**RM**  | average number of rooms per dwelling | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_study = ['ZN', 'INDUS', 'NOX', 'RM']\n",
    "\n",
    "sns.pairplot(dataF[col_study], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code   | Description   |\n",
    "|:---|:---|\n",
    "|**PTRATIO**  | pupil-teacher ratio by town | \n",
    "|**B**  | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town | \n",
    "|**LSTAT**  | % lower status of the population | \n",
    "|**Price**  | Median value of owner-occupied homes in \\$1000's | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_study = ['PTRATIO', 'B', 'LSTAT', 'Price']\n",
    "\n",
    "sns.pairplot(dataF[col_study], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transformation\n",
    "\n",
    "##### lets make a function that will create some categories\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['NOX'].min(),  dataF['NOX'].mean(),  dataF['NOX'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOX_category(nox):\n",
    "\n",
    "    if(nox >= 0.55):\n",
    "        NOX_cat = 'nox above mean'\n",
    "    else:\n",
    "        NOX_cat = 'nox below mean'    \n",
    "        \n",
    "    return NOX_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['NOX_category'] = dataF['NOX'].apply(NOX_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['NOX_category'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['NOX_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe we can do it simpler? \n",
    "#Take a look into the TAX column  (187,408,711)\n",
    "\n",
    "#TAX      full-value property-tax rate per $10,000\n",
    "\n",
    "dataF['Tax_category'] = ['tax above mean' if tax >= dataF['TAX'].mean() else 'tax above median' for tax in dataF['TAX']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['Tax_category'].head(),  dataF['Tax_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### analyze only subset of data - remove columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['CHAS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataF['CHAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### proper data types\n",
    "\n",
    "- There are three main data types:\n",
    "    - Numeric, e.g. income, age\n",
    "    - Categorical, e.g. gender, nationality \n",
    "    - Ordinal, e.g. low/medium/high\n",
    "    \n",
    "    \n",
    "- Models most of the time can only handle numeric features\n",
    "\n",
    "\n",
    "- Must convert categorical and ordinal features into numeric features\n",
    "    - Create dummy features\n",
    "    - Transform a categorical feature into a set of dummy features, each representing a unique category\n",
    "    - In the set of dummy features, 1 indicates that the observation belongs to that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which categorical variables you want to use in model\n",
    "for col_name in dataF.columns:\n",
    "    if dataF[col_name].dtypes == 'object':\n",
    "        unique_cat = len(dataF[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} unique categories\".format(col_name=col_name, unique_cat=unique_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dataF.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example - just to show how we can deal with them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = pd.Series([1,0,np.NAN,np.NAN,5,9,np.NaN])\n",
    "\n",
    "dt2 = pd.DataFrame ([ [1,      0,     np.NAN],\n",
    "                      [np.NAN, 0,     np.NAN],\n",
    "                      [6,      8,     2] ])\n",
    "\n",
    "dt3 = pd.DataFrame ([ [1,      0,     np.NAN],\n",
    "                      [np.NAN, 0,     np.NAN],\n",
    "                      [6,      8,     2] ])\n",
    "\n",
    "dt4 = pd.DataFrame ([ ['?',0,2],\n",
    "                      ['?',0,4],\n",
    "                      [6  ,8,3] ])\n",
    "\n",
    "\n",
    "dt5 = pd.DataFrame ([ [1,      0,     np.NAN],\n",
    "                      [np.NAN, 0,     np.NAN],\n",
    "                      [2,      8,     2]     ,\n",
    "                      [np.NAN, 0,     np.NAN],\n",
    "                      [3,      8,     2]     ,\n",
    "                      [np.NAN, 0,     np.NAN],\n",
    "                      [4,      8,     2]     ,\n",
    "                      [4,      8,     2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row level drop\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column level drop\n",
    "dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter cut-off\n",
    "dt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3.dropna(thresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change vale to NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4[dt4[0] == \"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4[0].replace(\"?\", np.NAN, inplace=True)\n",
    "dt4\n",
    "\n",
    "\n",
    "# dt4 = dt4.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using Imputer in sklearn.preprocessing\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "imp.fit(dt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt5 = pd.DataFrame(data=imp.transform(dt5) , columns=dt5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could use other approach as well:\n",
    "\n",
    "dt5.fillna(dt5.mean())  -- same as above, mean value\n",
    "\n",
    "dt5.fillna(0) -- constant value\n",
    "\n",
    "dt5.fillna(method='ffill')  --interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### removing duplicates\n",
    "\n",
    "- delete a row\n",
    "- delete by analyzing a specific column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = pd.DataFrame({'col1':[1,2,2,3,3,3,4,4,4,4],\n",
    "                     'col2':[1,1,1,1,1,2,2,2,2,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete a row -> entire data frame is analyzed\n",
    "\n",
    "dups.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete by analyzing a specified column\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups.drop_duplicates(['col1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "- An outlier is an observation that deviates drastically from other observations in a dataset\n",
    "\n",
    "\n",
    "- Occurrence:\n",
    "    - Natural, Bill Gates's income\n",
    "    - Error,  impossible value in the data set - Heart Rate 1681 (should be 168.1)\n",
    "\n",
    "\n",
    "- How bad it is\n",
    "    - Naturally occuring:\n",
    "        - Not necessarily problematic\n",
    "        - But can skew your model by affecting the slope  \n",
    "    - Error \n",
    "        - Indicative of data quality issues\n",
    "        - Treat in the same way as a missing value, i.e. use imputation\n",
    "   \n",
    "   \n",
    "- Many approaches for detecting outliers:\n",
    "    - Tukey IQR\n",
    "    - Kernel density estimatation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection - Tukey IQR\n",
    "- Identifies extreme values in data\n",
    "\n",
    "- Outliers are defined as:\n",
    "    - Values below Q1-1.5(Q3-Q1) or above Q3+1.5(Q3-Q1)\n",
    " \n",
    " \n",
    "- Standard deviation from the mean is another common method to detect extreme values\n",
    "    - But it can be problematic:\n",
    "        - Assumes normality \n",
    "        - Sensitive to very extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_tukey(x):\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "    iqr = q3-q1 \n",
    "    floor = q1 - 1.5*iqr\n",
    "    ceiling = q3 + 1.5*iqr\n",
    "    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n",
    "    outlier_values = list(x[outlier_indices])\n",
    "\n",
    "    return outlier_indices, outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF['CRIM'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey_indices, tukey_values = find_outliers_tukey(dataF['CRIM'])\n",
    "print(np.sort(tukey_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection - Kernel Density Estimation\n",
    "- Non-parametric way to estimate the probability density function of a given feature\n",
    "\n",
    "\n",
    "- Can be advantageous compared to extreme value detection (e.g. Tukey IQR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "\n",
    "def find_outliers_kde(x):\n",
    "    x_scaled = scale(list(map(float, x)))\n",
    "    kde = KDEUnivariate(x_scaled)\n",
    "    kde.fit(bw=\"scott\", fft=True)\n",
    "    pred = kde.evaluate(x_scaled)\n",
    "    \n",
    "    n = sum(pred < 0.05)\n",
    "    outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "    outlier_value = np.asarray(x)[outlier_ind]\n",
    "\n",
    "    return outlier_ind, outlier_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_indices, kde_values = find_outliers_kde(dataF['CRIM'])\n",
    "print(np.sort(kde_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(dataF.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(dataF[['CRIM', 'ZN', 'INDUS',  'Price']].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelF = pd.DataFrame ([ ['XS'],['S'],['M'],['L'],['XL'],['XXL'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelF['encoded'] = encoder.fit_transform(labelF[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder or Dummy Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict =np.array (['A','B','C','D','E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = dict.reshape(len(dict),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEncoded = encoder.fit_transform(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEncoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scaling, normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the minimum and maximum to be used for later scaling\n",
    "scaler.fit(dataF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(dataF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of error - we have 2 columns that stores categorical data\n",
    "Scaling does not allow that\n",
    "drop the column or create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataF['NOX_category']\n",
    "\n",
    "del dataF['Tax_category']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the mean and std to be used for later scaling\n",
    "scaler.fit(dataF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(dataF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do nothing and return the estimator unchanged\n",
    "transformer.fit(dataF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(dataF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it is calulated?\n",
    "x0 = take the cell value \n",
    "norm0 =calculate ABS(sum all values in the row)\n",
    "\n",
    "x0 \\ norm0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do nothing and return the estimator unchanged\n",
    "transformer = Binarizer().fit(dataF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(dataF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.set_params(threshold=7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(dataF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Statistical procedure that utilise orthogonal transformation technology\n",
    "\n",
    "Convert possible correlated features (predictors) into linearly uncorrelated features (predictors) called principal components of principal components <= number of features (predictors)\n",
    "\n",
    "First principal component explains the largest possible variance\n",
    "\n",
    "Each subsequent component has the highest variance subject to the restriction that it must be orthogonal to the preceding components.\n",
    "\n",
    "A collection of the components are called vectors\n",
    "\n",
    "Sensitive to scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataF.iloc[:, 0:12]\n",
    "y = dataF['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.round(pca.components_, 3), columns=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "pca.fit(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pca.transform(X_train_sc)\n",
    "index_name = ['PCA_'+str(k) for k in range(0, len(res))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(res, columns=dataF.columns[0:12],\n",
    "                   index=index_name)[0:4]\n",
    "df1.T.sort_values(by='PCA_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
